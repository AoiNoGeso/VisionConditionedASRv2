import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.amp import autocast, GradScaler
from transformers import AutoTokenizer
import os
from datetime import datetime
from dataclasses import dataclass
from typing import Optional, List
import numpy as np
from tqdm import tqdm
import wandb
from safetensors.torch import save_file, load_file
from audiomentations import Compose, AddGaussianSNR, AddBackgroundNoise, AddGaussianNoise

from model_DINOv2 import VisionConditionedASR
from dataloader import create_dataloader


@dataclass
class TrainingConfig:
    """å­¦ç¿’è¨­å®š"""
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹
    train_json: str = "../../Datasets/SpokenCOCO/SpokenCOCO_train_fixed.json"
    val_json: str = "../../Datasets/SpokenCOCO/SpokenCOCO_val_fixed.json"
    audio_dir: str = "../../Datasets/SpokenCOCO"
    image_dir: str = "../../Datasets/stair_captions/images"
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    vocab_size: Optional[int] = None
    hidden_dim: int = 256
    num_heads: int = 4
    
    # å­¦ç¿’è¨­å®š
    batch_size: int = 16
    num_epochs: int = 10
    learning_rate: float = 1e-5
    weight_decay: float = 1e-5
    gradient_clip: float = 1.0
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼è¨­å®š
    num_workers: int = 4
    max_audio_length: float = 10.0
    validate_files: bool = True
    
    # å±¤å‡çµè¨­å®š
    freeze_audio_encoder: bool = False
    freeze_vision_encoder: bool = True
    freeze_cross_attention: bool = False

    # ãƒã‚¤ã‚ºè¨­å®š
    noise_type: str = "none"  # "none", "gaussian", "white", "background"
    noise_dir: Optional[str] = "../../Datasets/noise"  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    gaussian_snr_db: tuple = (5, 20)  # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã®SNRç¯„å›²(dB)
    white_noise_level: float = 0.01  # ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºã®ãƒ¬ãƒ™ãƒ« (0.0-1.0)
    background_snr_db: tuple = (5, 20)  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºã®SNRç¯„å›²(dB)
    
    # å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    warmup_steps: int = 1000
    
    # åŠç²¾åº¦å­¦ç¿’è¨­å®š
    use_amp: bool = True  # Automatic Mixed Precision (AMP) ã®ä½¿ç”¨
    amp_dtype: str = "float16"  # "float16" or "bfloat16"
    
    # ä¿å­˜è¨­å®š
    checkpoint_dir: str = "../checkpoints/DINOv2_model"
    save_epoch: int = 1
    
    # å­¦ç¿’å†é–‹è¨­å®š
    resume_from: Optional[str] = None # å†é–‹ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ä¾‹: "../checkpoints/pure_cross/epoch_5")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device: str = "cuda:1"  # "cuda:0", "cuda:1", "cpu"
    
    # ãƒ­ã‚°è¨­å®š
    log_step: int = 50  # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨
    validate_epoch: int = 1  # ã‚¨ãƒãƒƒã‚¯ã”ã¨
    use_wandb: bool = True  # wandbã®ä½¿ç”¨/ä¸ä½¿ç”¨
    wandb_project: str = "VisionConditionedASR"


class NoiseAugmenter:
    """éŸ³å£°ã«ãƒã‚¤ã‚ºã‚’ä»˜åŠ ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, noise_type: str = "none", gaussian_snr_db: tuple = (5, 20),
                 white_noise_level: float = 0.01, noise_dir: Optional[str] = None, 
                 background_snr_db: tuple = (5, 20), sample_rate: int = 16000):
        """
        Args:
            noise_type: ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ— ("none", "gaussian", "white", "background")
            gaussian_snr_db: ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã®SNRç¯„å›²(dB)
            white_noise_level: ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºã®ãƒ¬ãƒ™ãƒ«
            noise_dir: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            background_snr_db: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºã®SNRç¯„å›²(dB)
            sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        """
        self.noise_type = noise_type
        self.sample_rate = sample_rate
        
        print(f"\n[NoiseAugmenter] Initializing with noise_type: {noise_type}")
        
        if noise_type == "none":
            self.augment = None
            print("  No noise augmentation")
            
        elif noise_type == "gaussian":
            self.augment = Compose([
                AddGaussianSNR(min_snr_db=gaussian_snr_db[0], max_snr_db=gaussian_snr_db[1], p=1.0)
            ])
            print(f"  Gaussian noise SNR: {gaussian_snr_db[0]}-{gaussian_snr_db[1]} dB")
            
        elif noise_type == "white":
            # ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºã¯å‡ä¸€åˆ†å¸ƒã®ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã¨ã—ã¦å®Ÿè£…
            self.augment = Compose([
                AddGaussianNoise(min_amplitude=white_noise_level, max_amplitude=white_noise_level, p=1.0)
            ])
            print(f"  White noise level: {white_noise_level}")
            
        elif noise_type == "background":
            if noise_dir is None or not os.path.exists(noise_dir):
                raise ValueError(f"Background noise directory not found: {noise_dir}")
            
            # ãƒã‚¤ã‚ºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
            noise_files = [os.path.join(noise_dir, f) for f in os.listdir(noise_dir) 
                          if f.endswith(('.wav', '.mp3', '.flac'))]
            
            if len(noise_files) == 0:
                raise ValueError(f"No noise files found in: {noise_dir}")
            
            self.augment = Compose([
                AddBackgroundNoise(
                    sounds_path=noise_dir,
                    min_snr_in_db=background_snr_db[0],
                    max_snr_in_db=background_snr_db[1],
                    p=1.0
                )
            ])
            print(f"  Background noise from: {noise_dir}")
            print(f"  Found {len(noise_files)} noise files")
            print(f"  SNR range: {background_snr_db[0]}-{background_snr_db[1]} dB")
            
        else:
            raise ValueError(f"Unknown noise_type: {noise_type}")
    
    def apply(self, audio: np.ndarray) -> np.ndarray:
        """
        éŸ³å£°ã«ãƒã‚¤ã‚ºã‚’ä»˜åŠ 
        
        Args:
            audio: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ (numpy array, shape: (T,))
        
        Returns:
            ãƒã‚¤ã‚ºãŒä»˜åŠ ã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿
        """
        if self.augment is None:
            return audio
        
        # audiomentationsã¯float32ã‚’æœŸå¾…
        if audio.dtype != np.float32:
            audio = audio.astype(np.float32)
        
        # ãƒã‚¤ã‚ºã‚’ä»˜åŠ 
        augmented_audio = self.augment(samples=audio, sample_rate=self.sample_rate)
        
        return augmented_audio


def freeze_layers(model: VisionConditionedASR, config: TrainingConfig):
    """
    æŒ‡å®šã•ã‚ŒãŸå±¤ã‚’å‡çµ
    
    Args:
        model: VisionConditionedASRãƒ¢ãƒ‡ãƒ«
        config: å­¦ç¿’è¨­å®š
    """
    print("\n" + "="*60)
    print("Layer Freeze Configuration")
    print("="*60)
    
    # Audio Encoderã®å‡çµ
    if config.freeze_audio_encoder:
        for param in model.audio_encoder.model.parameters():
            param.requires_grad = False
        print("âœ“ Audio Encoder (Wav2Vec2):    FROZEN")
    else:
        print("âœ“ Audio Encoder (Wav2Vec2):    Trainable")
    
    # Vision Encoderã®å‡çµ
    if config.freeze_vision_encoder:
        for param in model.vision_encoder.model.vision_model.parameters():
            param.requires_grad = False
        print("âœ“ Vision Encoder (CLIP):       FROZEN")
    else:
        print("âœ“ Vision Encoder (CLIP):       Trainable")
    
    # Cross Attentionã®å‡çµ
    if config.freeze_cross_attention:
        for param in model.cross_attention.parameters():
            param.requires_grad = False
        print("âœ“ Cross Attention:             FROZEN")
    else:
        print("âœ“ Cross Attention:             Trainable")
    
    # Classifierã¯å¸¸ã«å­¦ç¿’å¯èƒ½
    print("âœ“ Classifier (Linear):         Trainable (always)")
    
    # å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    frozen_params = total_params - trainable_params
    
    print(f"\n{'='*60}")
    print("Parameter Statistics:")
    print(f"{'='*60}")
    print(f"Trainable parameters:  {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    print(f"Frozen parameters:     {frozen_params:,} ({100 * frozen_params / total_params:.2f}%)")
    print(f"Total parameters:      {total_params:,}")
    print(f"{'='*60}\n")


def decode_predictions(
    predicted_ids: torch.Tensor, 
    tokenizer,
    blank_token_id: int = 0
) -> List[str]:
    """
    CTCã®äºˆæ¸¬çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ‡ã‚³ãƒ¼ãƒ‰
    
    Args:
        predicted_ids: äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ID [batch, seq_len]
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        blank_token_id: CTCã®blankãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆé€šå¸¸ã¯0ï¼‰
        
    Returns:
        ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    decoded_texts = []
    
    for pred_ids_seq in predicted_ids:
        pred_tokens = []
        prev_token = None
        
        for token_id in pred_ids_seq.tolist():
            # blank tokenã¯ã‚¹ã‚­ãƒƒãƒ—
            if token_id == blank_token_id:
                prev_token = None
                continue
            
            # é€£ç¶šã™ã‚‹åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã¯1ã¤ã ã‘ä¿æŒ
            if token_id != prev_token:
                pred_tokens.append(token_id)
            
            prev_token = token_id
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        decoded_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)
        decoded_texts.append(decoded_text)
    
    return decoded_texts


def compute_ctc_loss(
    logits: torch.Tensor,
    texts: List[str],
    tokenizer,
    wav_lengths: torch.Tensor
) -> torch.Tensor:
    """
    CTCæå¤±ã‚’è¨ˆç®—
    
    Args:
        logits: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ› [B, T, vocab_size]
        texts: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆ [B]
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        wav_lengths: å„éŸ³å£°ã®é•·ã• [B]
    
    Returns:
        loss: CTCæå¤±
    """
    batch_size = logits.size(0)
    device = logits.device
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    target_ids = []
    target_lengths = []
    
    for text in texts:
        tokens = tokenizer.encode(text, add_special_tokens=False)
        target_ids.extend(tokens)
        target_lengths.append(len(tokens))
    
    # Tensorã«å¤‰æ›
    target_ids = torch.tensor(target_ids, dtype=torch.long, device=device)
    target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)
    
    # å…¥åŠ›é•·ã‚’è¨ˆç®—
    input_lengths = torch.full((batch_size,), logits.size(1), dtype=torch.long, device=device)
    
    # CTCLossã®è¨ˆç®—
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
    log_probs = log_probs.transpose(0, 1)  # [T, B, vocab_size]
    
    ctc_loss = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)
    
    try:
        loss = ctc_loss(log_probs, target_ids, input_lengths, target_lengths)
    except RuntimeError as e:
        print(f"\n[Warning] CTC Loss calculation error: {e}")
        print(f"  Input lengths: {input_lengths.tolist()}")
        print(f"  Target lengths: {target_lengths.tolist()}")
        loss = torch.tensor(1e6, device=device, requires_grad=True)
    
    return loss


def train_one_epoch(
    model: VisionConditionedASR,
    noise_augmenter: NoiseAugmenter,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scaler: GradScaler,
    tokenizer,
    device: torch.device,
    epoch: int,
    config: TrainingConfig
):
    """
    1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        noise_augmenter: ãƒã‚¤ã‚ºä»˜åŠ å™¨
        dataloader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        scaler: GradScaler (AMPç”¨)
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        device: ãƒ‡ãƒã‚¤ã‚¹
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·
        config: å­¦ç¿’è¨­å®š
    
    Returns:
        avg_loss: å¹³å‡æå¤±
    """
    model.train()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    # amp_dtypeã®è¨­å®š
    amp_dtype = torch.float16 if config.amp_dtype == "float16" else torch.bfloat16
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1}/{config.num_epochs} - Training")
    print(f"{'='*60}")
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1} Train", total=num_batches)
    
    for batch_idx, batch in enumerate(pbar):
        try:
            # ãƒã‚¤ã‚ºã‚’ä»˜åŠ 
            if noise_augmenter.augment is not None:
                noisy_wav = []
                for wav in batch["wav"]:
                    augmented = noise_augmenter.apply(wav)
                    noisy_wav.append(augmented)
                batch["wav"] = noisy_wav

            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ï¼ˆwav_lengthsã®ã¿ï¼‰
            wav_lengths = batch["wav_lengths"].to(device)
            
            # Forward pass with autocast
            with autocast(device_type='cuda', enabled=config.use_amp, dtype=amp_dtype):
                logits = model(batch)  # [B, T, vocab_size]
                
                # NaN/Infãƒã‚§ãƒƒã‚¯
                if torch.isnan(logits).any() or torch.isinf(logits).any():
                    print(f"\nğŸš¨ CRITICAL: Logits contain NaN or Inf at batch {batch_idx}!")
                    print(f"  Skipping this batch...")
                    continue
                
                # CTCæå¤±ã®è¨ˆç®—
                loss = compute_ctc_loss(logits, batch["text"], tokenizer, wav_lengths)
            
            # NaN/Infãƒã‚§ãƒƒã‚¯
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\nğŸš¨ CRITICAL: Loss is NaN or Inf at batch {batch_idx}!")
                print(f"  Skipping this batch...")
                continue
            
            # Backward pass with scaler
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            
            # Gradient clipping
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)
            
            # Optimizer step
            scaler.step(optimizer)
            scaler.update()
            
            # æå¤±ã®ç´¯ç©
            current_loss = loss.item()
            total_loss += current_loss
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«æå¤±ã‚’è¡¨ç¤º
            pbar.set_postfix(loss=f"{current_loss:.4f}")
            
            # ãƒ­ã‚°å‡ºåŠ›
            if (batch_idx + 1) % config.log_step == 0 or (batch_idx + 1) == num_batches:
                avg_loss = total_loss / (batch_idx + 1)

                # wandbã«ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®æå¤±ã‚’ãƒ­ã‚°
                if config.use_wandb:
                    wandb.log({
                        "train/loss_step": current_loss,
                        "train/avg_loss_step": avg_loss,
                        "train/scale": scaler.get_scale(),
                        "epoch": epoch,
                    }, step=epoch * num_batches + batch_idx + 1)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if batch_idx % 50 == 0 and device.type == 'cuda':
                torch.cuda.empty_cache()
        
        except Exception as e:
            print(f"\n[Error] Exception at batch {batch_idx}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    
    # wandbã«ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®å¹³å‡æå¤±ã‚’ãƒ­ã‚°
    if config.use_wandb:
        wandb.log({
            "train/loss_epoch": avg_loss,
            "epoch": epoch,
        })
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1} Training Summary:")
    print(f"  Average Loss: {avg_loss:.4f}")
    print(f"{'='*60}\n")
    
    return avg_loss


def validate(
    model: VisionConditionedASR,
    noise_augmenter: NoiseAugmenter,
    dataloader: DataLoader,
    tokenizer,
    device: torch.device,
    epoch: int,
    config: TrainingConfig,
    num_examples: int = 3
):
    """
    æ¤œè¨¼
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        noise_augmenter: ãƒã‚¤ã‚ºä»˜åŠ å™¨
        dataloader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        device: ãƒ‡ãƒã‚¤ã‚¹
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·
        config: å­¦ç¿’è¨­å®š
        num_examples: è¡¨ç¤ºã™ã‚‹äºˆæ¸¬ä¾‹ã®æ•°
    
    Returns:
        avg_loss: å¹³å‡æå¤±
    """
    model.eval()
    total_loss = 0.0
    num_batches = 0
    all_predictions = []
    all_references = []
    
    # amp_dtypeã®è¨­å®š
    amp_dtype = torch.float16 if config.amp_dtype == "float16" else torch.bfloat16
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1}/{config.num_epochs} - Validation")
    print(f"{'='*60}")
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1} Val", total=len(dataloader))
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(pbar):
            try:
                # ãƒã‚¤ã‚ºã‚’ä»˜åŠ 
                if noise_augmenter.augment is not None:
                    noisy_wav = []
                    for wav in batch["wav"]:
                        augmented = noise_augmenter.apply(wav)
                        noisy_wav.append(augmented)
                    batch["wav"] = noisy_wav

                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                wav_lengths = batch["wav_lengths"].to(device)
                
                # Forward pass with autocast
                with autocast(device_type='cuda', enabled=config.use_amp, dtype=amp_dtype):
                    logits = model(batch)
                    
                    # æå¤±è¨ˆç®—
                    loss = compute_ctc_loss(logits, batch["text"], tokenizer, wav_lengths)
                
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    current_loss = loss.item()
                    total_loss += current_loss
                    num_batches += 1
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«æå¤±ã‚’è¡¨ç¤º
                    pbar.set_postfix(loss=f"{current_loss:.4f}")
                
                # äºˆæ¸¬ã®ãƒ‡ã‚³ãƒ¼ãƒ‰
                predicted_ids = torch.argmax(logits, dim=-1)
                pred_texts = decode_predictions(predicted_ids, tokenizer, blank_token_id=0)
                
                # çµæœã®ä¿å­˜
                all_predictions.extend(pred_texts)
                all_references.extend(batch["text"])
                
            except Exception as e:
                print(f"\n[Error] Exception at validation batch {batch_idx}: {e}")
                continue
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    
    # äºˆæ¸¬ä¾‹ã‚’è¡¨ç¤º
    print(f"\n{'='*60}")
    print("Prediction Examples:")
    print(f"{'='*60}")
    
    prediction_table = []
    for i in range(min(num_examples, len(all_predictions))):
        ref = all_references[i][:80]
        pred = all_predictions[i][:80]
        print(f"\nExample {i+1}:")
        print(f"  Reference:  {ref}")
        print(f"  Prediction: {pred}")
        prediction_table.append([i+1, ref, pred])
        
    # wandbã«æ¤œè¨¼çµæœã‚’ãƒ­ã‚°
    if config.use_wandb:
        wandb.log({
            "val/loss_epoch": avg_loss,
            "val/prediction_examples": wandb.Table(
                data=prediction_table, 
                columns=["Example", "Reference", "Prediction"]
            ),
            "epoch": epoch,
        })
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1} Validation Summary:")
    print(f"  Average Loss: {avg_loss:.4f}")
    print(f"  Total samples: {len(all_predictions)}")
    print(f"{'='*60}\n")
    
    return avg_loss


def save_checkpoint(
    model: VisionConditionedASR,
    optimizer: torch.optim.Optimizer,
    scaler: GradScaler,
    epoch: int,
    train_loss: float,
    val_loss: float,
    config: TrainingConfig
):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        scaler: GradScaler (AMPç”¨)
        epoch: ã‚¨ãƒãƒƒã‚¯ç•ªå·
        train_loss: è¨“ç·´æå¤±
        val_loss: æ¤œè¨¼æå¤±
        config: å­¦ç¿’è¨­å®š
    """
    # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    epoch_dir = os.path.join(config.checkpoint_dir, f"epoch_{epoch+1}")
    os.makedirs(epoch_dir, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’.safetensorså½¢å¼ã§ä¿å­˜
    model_path = os.path.join(epoch_dir, f"model_epoch_{epoch+1}.safetensors")
    save_file(model.state_dict(), model_path)
    
    # ãã®ä»–ã®å­¦ç¿’çŠ¶æ…‹ã‚’.ptå½¢å¼ã§ä¿å­˜
    state_path = os.path.join(epoch_dir, f"checkpoint_epoch_{epoch+1}_state.pt")
    torch.save({
        'epoch': epoch,
        'optimizer_state_dict': optimizer.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'config': config
    }, state_path)
    
    print(f"[Checkpoint] Model saved to: {model_path}")
    print(f"[Checkpoint] State saved to: {state_path}")


def load_checkpoint(
    checkpoint_dir: str,
    model: VisionConditionedASR,
    optimizer: torch.optim.Optimizer,
    scaler: GradScaler,
    device: torch.device
):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å­¦ç¿’ã‚’å†é–‹
    
    Args:
        checkpoint_dir: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ (ä¾‹: ../checkpoints/epoch_3)
        model: ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        scaler: GradScaler
        device: ãƒ‡ãƒã‚¤ã‚¹
    
    Returns:
        start_epoch: å†é–‹ã™ã‚‹ã‚¨ãƒãƒƒã‚¯ç•ªå·
        best_val_loss: ä¿å­˜ã•ã‚Œã¦ã„ãŸæœ€è‰¯ã®æ¤œè¨¼æå¤±
    """
    if not os.path.exists(checkpoint_dir):
        raise FileNotFoundError(f"Checkpoint directory not found: {checkpoint_dir}")
    
    # ã‚¨ãƒãƒƒã‚¯ç•ªå·ã‚’å–å¾—
    epoch_num = os.path.basename(checkpoint_dir).split('_')[-1]
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    model_path = os.path.join(checkpoint_dir, f"model_epoch_{epoch_num}.safetensors")
    state_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch_num}_state.pt")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    if not os.path.exists(state_path):
        raise FileNotFoundError(f"State file not found: {state_path}")
    
    print(f"\n{'='*60}")
    print("Resuming Training from Checkpoint")
    print(f"{'='*60}")
    print(f"Loading from: {checkpoint_dir}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆsafetensorsï¼‰
    state_dict = load_file(model_path, device=str(device))
    model.load_state_dict(state_dict)
    
    # å­¦ç¿’çŠ¶æ…‹ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆ.ptï¼‰
    checkpoint_state = torch.load(state_path, map_location=device, weights_only=False)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹ã‚’å¾©å…ƒ
    optimizer.load_state_dict(checkpoint_state['optimizer_state_dict'])
    
    # GradScalerã®çŠ¶æ…‹ã‚’å¾©å…ƒ
    if 'scaler_state_dict' in checkpoint_state:
        scaler.load_state_dict(checkpoint_state['scaler_state_dict'])
    
    # ã‚¨ãƒãƒƒã‚¯æƒ…å ±ã‚’å–å¾—
    start_epoch = checkpoint_state['epoch'] + 1
    train_loss = checkpoint_state.get('train_loss', 0.0)
    val_loss = checkpoint_state.get('val_loss', 0.0)
    
    print(f"[Resume] Successfully loaded checkpoint")
    print(f"  Last completed epoch: {checkpoint_state['epoch'] + 1}")
    print(f"  Resuming from epoch: {start_epoch + 1}")
    print(f"  Last train loss: {train_loss:.4f}")
    print(f"  Last val loss: {val_loss:.4f}")
    print(f"{'='*60}\n")
    
    return start_epoch, val_loss


def main():
    """ãƒ¡ã‚¤ãƒ³å­¦ç¿’é–¢æ•°"""
    # è¨­å®šã®åˆæœŸåŒ–
    config = TrainingConfig()
    
    # wandbã®åˆæœŸåŒ–
    if config.use_wandb:
        print("[Setup] Initializing wandb...")
        wandb.init(
            project=config.wandb_project,
            config=config.__dict__
        )
    
    # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
    device = torch.device(config.device if torch.cuda.is_available() else "cpu")
    print(f"\n{'='*60}")
    print("Training Configuration")
    print(f"{'='*60}")
    print(f"Device: {device}")
    print(f"Batch size: {config.batch_size}")
    print(f"Learning rate: {config.learning_rate}")
    print(f"Num epochs: {config.num_epochs}")
    print(f"Noise type: {config.noise_type}")
    print(f"Use AMP: {config.use_amp}")
    if config.use_amp:
        print(f"AMP dtype: {config.amp_dtype}")
    print(f"Use wandb: {config.use_wandb}")
    print(f"{'='*60}\n")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    print("[Setup] Initializing model...")
    model = VisionConditionedASR(
        vocab_size=config.vocab_size,
        hidden_dim=config.hidden_dim,
        num_heads=config.num_heads,
        device=device
    ).to(device)
    
    # å±¤ã®å‡çµ
    freeze_layers(model, config)
    
    # GradScalerã®åˆæœŸåŒ–
    scaler = GradScaler(enabled=config.use_amp)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    print("[Setup] Creating dataloaders...")
    train_loader = create_dataloader(
        json_path=config.train_json,
        audio_dir=config.audio_dir,
        image_dir=config.image_dir,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        max_audio_length=config.max_audio_length,
        validate_files=config.validate_files
    )
    
    val_loader = create_dataloader(
        json_path=config.val_json,
        audio_dir=config.audio_dir,
        image_dir=config.image_dir,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        max_audio_length=config.max_audio_length,
        validate_files=config.validate_files
    )
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®è¨­å®š
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    # ãƒã‚¤ã‚ºä»˜åŠ å™¨ã®åˆæœŸåŒ–
    print("[Setup] Initializing noise augmenter...")
    noise_augmenter = NoiseAugmenter(
        noise_type=config.noise_type,
        gaussian_snr_db=config.gaussian_snr_db,
        white_noise_level=config.white_noise_level,
        noise_dir=config.noise_dir,
        background_snr_db=config.background_snr_db,
        sample_rate=16000
    )
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print("\n" + "="*60)
    print("Starting Training")
    print("="*60 + "\n")
    
    best_val_loss = float('inf')
    start_epoch = 0
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹å ´åˆ
    if config.resume_from is not None:
        start_epoch, best_val_loss = load_checkpoint(
            config.resume_from, model, optimizer, scaler, device
        )
        print(f"[Resume] Best validation loss so far: {best_val_loss:.4f}\n")
    
    for epoch in range(start_epoch, config.num_epochs):
        # è¨“ç·´
        train_loss = train_one_epoch(
            model, noise_augmenter, train_loader, optimizer, scaler, tokenizer, device, epoch, config
        )
        
        # æ¤œè¨¼
        val_loss = 0.0
        if (epoch + 1) % config.validate_epoch == 0:
            val_loss = validate(
                model, noise_augmenter, val_loader, tokenizer, device, epoch, config
            )
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                print(f"\nâœ¨ New best validation loss: {best_val_loss:.4f}")
                save_checkpoint(
                    model, optimizer, scaler, epoch, train_loss, val_loss, config
                )
        
        # å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if (epoch + 1) % config.save_epoch == 0:
            save_checkpoint(
                model, optimizer, scaler, epoch, train_loss, val_loss, config
            )
    
    print("\n" + "="*60)
    print("Training Completed!")
    print("="*60 + "\n")
    
    # wandbã®çµ‚äº†
    if config.use_wandb:
        wandb.finish()


if __name__ == "__main__":
    main()